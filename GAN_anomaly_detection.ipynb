{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1a4ed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.preprocessing.label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m data_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIRECTORY, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_data_full.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 43\u001b[0m     network_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Extract training and testing datasets\u001b[39;00m\n\u001b[0;32m     46\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m network_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.preprocessing.label'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "# Importing data handling and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import machine learning and deep learning libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, Input, BatchNormalization, Reshape, Flatten, Activation\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# here we setting a random seed for reproducability\n",
    "random.seed(42)\n",
    "\n",
    "# Data directory containing the KDD dataset\n",
    "DATA_DIRECTORY = './data/'\n",
    "\n",
    "# Function to normalize data the into L2-norm for easier processing\n",
    "def normalize_dataset(data):\n",
    "    return data / np.linalg.norm(data, axis=0)\n",
    "\n",
    "# Loading the preprocessed KDD dataset. The data set is created into a  pickle file\n",
    "data_file_path = os.path.join(DATA_DIRECTORY, 'preprocessed_data_full.pkl')\n",
    "with open(data_file_path, 'rb') as file:\n",
    "    network_data = pickle.load(file)\n",
    "\n",
    "# here we are Extracting the training and testing datasets\n",
    "label_encoder = network_data['le']\n",
    "x_train_data = network_data['x_train']\n",
    "y_train_data = network_data['y_train']\n",
    "x_test_data = network_data['x_test']\n",
    "y_test_data = network_data['y_test']\n",
    "\n",
    "# Convert labels to binary for anomaly detection\n",
    "y_train_data = np.where(y_train_data != 11, 1, 0)\n",
    "y_test_data = np.where(y_test_data != 11, 1, 0)\n",
    "\n",
    "# Subset only normal network packets for training\n",
    "normal_data = x_train_data[y_train_data == 0]\n",
    "\n",
    "# Scale data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_x_train = scaler.fit_transform(normal_data)\n",
    "scaled_x_test = scaler.transform(x_test_data)\n",
    "\n",
    "# Organize dataset into a dictionary\n",
    "processed_dataset = {\n",
    "    'x_train': scaled_x_train.astype(np.float32),\n",
    "    'y_train': y_train_data.astype(np.float32),\n",
    "    'x_test': scaled_x_test.astype(np.float32),\n",
    "    'y_test': y_test_data.astype(np.float32)\n",
    "}\n",
    "\n",
    "# Defining the generator model \n",
    "def create_generator_model(optimizer):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=114, kernel_initializer=initializers.GlorotNormal(seed=42)),\n",
    "        Activation('tanh'),\n",
    "        Dense(128),\n",
    "        Activation('tanh'),\n",
    "        Dense(256),\n",
    "        Activation('tanh'),\n",
    "        Dense(114, activation='tanh')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def create_discriminator_model(optimizer):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=114, kernel_initializer=initializers.GlorotNormal(seed=42)),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dropout(0.2),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Define GAN model\n",
    "def create_gan_model(generator, discriminator, optimizer):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(114,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan_model = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan_model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan_model\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 10\n",
    "adam_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "\n",
    "# Initialize models\n",
    "generator = create_generator_model(adam_optimizer)\n",
    "discriminator = create_discriminator_model(adam_optimizer)\n",
    "gan_model = create_gan_model(generator, discriminator, adam_optimizer)\n",
    "\n",
    "# Training loop\n",
    "gan_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in range(scaled_x_train.shape[0] // BATCH_SIZE):\n",
    "        noise = np.random.normal(0, 1, size=(BATCH_SIZE, 114))\n",
    "        fake_data = generator.predict_on_batch(noise)\n",
    "        real_data = scaled_x_train[batch * BATCH_SIZE:(batch + 1) * BATCH_SIZE]\n",
    "        \n",
    "        X = np.vstack([fake_data, real_data])\n",
    "        y_dis = np.hstack([np.zeros(BATCH_SIZE), np.ones(BATCH_SIZE)])\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        noise = np.random.normal(0, 1, size=(BATCH_SIZE, 114))\n",
    "        y_gen = np.ones(BATCH_SIZE)\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan_model.train_on_batch(noise, y_gen)\n",
    "\n",
    "        discriminator_losses.append(d_loss)\n",
    "        gan_losses.append(g_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
    "\n",
    "# Plot training losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(discriminator_losses, label='Discriminator Loss', color='red')\n",
    "plt.plot(gan_losses, label='Generator Loss', color='blue')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Batch Count')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model on test set\n",
    "results = discriminator.predict(scaled_x_test)\n",
    "threshold = np.percentile(results, 1)\n",
    "\n",
    "predictions = (results > threshold).astype(int)\n",
    "accuracy = accuracy_score(processed_dataset['y_test'], predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(processed_dataset['y_test'], predictions, average='binary')\n",
    "\n",
    "print(f\"Accuracy is: {accuracy}\")\n",
    "print(f\"Precision is: {precision}\")\n",
    "print(f\"Recall is: {recall}\")\n",
    "print(f\"F1 Score is: {f1}\")\n",
    "\n",
    "# Trying to create the confusion matrix\n",
    "cm = confusion_matrix(processed_dataset['y_test'], predictions)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Normal', 'Anomalous'])\n",
    "plt.yticks([0, 1], ['Normal', 'Anomalous'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(processed_dataset['y_test'], results)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f718fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a089a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
